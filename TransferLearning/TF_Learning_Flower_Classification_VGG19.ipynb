{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_CKgJtUTgGd"
      },
      "outputs": [],
      "source": [
        "# data visualization and manipulation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# model selection \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve,roc_auc_score, ConfusionMatrixDisplay, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# preprocess\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.optimizers import Adam, SGD, Adagrad, Adadelta, RMSprop\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import tensorflow as tf\n",
        "import random as rn\n",
        "\n",
        "import cv2\n",
        "import numpy as np  \n",
        "from tqdm import tqdm\n",
        "import os                   \n",
        "from random import shuffle  \n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization,AveragePooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.python.keras.layers.convolutional import Conv2D, MaxPooling2D \n",
        "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "path_daisy = \"drive/MyDrive/flowers/daisy\"\n",
        "path_dandelion = \"drive/MyDrive/flowers/dandelion\"\n",
        "path_rose = \"drive/MyDrive/flowers/rose\"\n",
        "path_sunflower = \"drive/MyDrive/flowers/sunflower\"\n",
        "path_tulip = \"drive/MyDrive/flowers/tulip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6T96P7LTmf7",
        "outputId": "1d610921-6a17-4320-cd3f-a2e22d52d131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_label(img, flower_type):\n",
        "    return flower_type\n",
        "\n",
        "def make_train_data(flower_type,DIR):\n",
        "    for img in tqdm(os.listdir(DIR)):\n",
        "        label = assign_label(img,flower_type)\n",
        "        path = os.path.join(DIR,img)\n",
        "        img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "        img = cv2.resize(img, (160,160))\n",
        "        \n",
        "        X.append(np.array(img))\n",
        "        Z.append(str(label))"
      ],
      "metadata": {
        "id": "zTXFngCjTmjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "Z = []\n",
        "\n",
        "make_train_data(\"Daisy\",path_daisy)\n",
        "print(len(X))\n",
        "\n",
        "make_train_data(\"Dandelion\",path_dandelion)\n",
        "print(len(X))\n",
        "\n",
        "make_train_data(\"Rose\",path_rose)\n",
        "print(len(X))\n",
        "\n",
        "make_train_data(\"Sunflower\",path_sunflower)\n",
        "print(len(X))\n",
        "\n",
        "make_train_data(\"Tulip\",path_tulip)\n",
        "print(len(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIG1CEe4Tml_",
        "outputId": "36c70ce0-7a1f-4b08-c467-33e9ee3e8e6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 764/764 [00:11<00:00, 67.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1052/1052 [00:10<00:00, 97.28it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 784/784 [00:06<00:00, 130.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 733/733 [00:06<00:00, 105.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 984/984 [00:06<00:00, 143.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoding the Y array (i.e. Daisy->0, Rose->1 etc...) & then One Hot Encoding\n",
        "labelEncoder = LabelEncoder()\n",
        "Y = labelEncoder.fit_transform(Z)\n",
        "Y = to_categorical(Y,5)\n",
        "X = np.array(X)\n",
        "X = X/ 255"
      ],
      "metadata": {
        "id": "M1VD8WI5Tmoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test, z_train, z_test = train_test_split(X, Y, Z, test_size = 0.2, random_state=42)\n",
        "\n",
        "print(\"x_train shape: \", x_train.shape)\n",
        "print(\"x_test shape: \", x_test.shape)\n",
        "print(\"y_train shape: \", y_train.shape)\n",
        "print(\"y_test shape: \", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDMCBktZUFtP",
        "outputId": "d295f606-e009-4aa3-f0ca-c9743fd505a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape:  (3453, 160, 160, 3)\n",
            "x_test shape:  (864, 160, 160, 3)\n",
            "y_train shape:  (3453, 5)\n",
            "y_test shape:  (864, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,   # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,   # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization= False,   # divide each input by its std\n",
        "    zca_whitening=False,   # dimesion reduction\n",
        "    rotation_range=10,    # randomly rotate images in the range 10 degrees\n",
        "    zoom_range=0.1,      # Randomly zoom image 10%\n",
        "    width_shift_range=0.2,   # randomly shift images horizontally 20%\n",
        "    height_shift_range=0.2,   # randomly shift images vertically 20%\n",
        "    horizontal_flip=True,     # randomly flip images\n",
        "    vertical_flip=False    # randomly flip images\n",
        ")\n",
        "datagen.fit(x_train)"
      ],
      "metadata": {
        "id": "PxVnyVGaUFwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = VGG19(input_shape = (160,160,3), weights='imagenet', include_top=False)\n",
        "for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        " \n",
        "# Get base model output \n",
        "base_model_ouput = base_model.output\n",
        "    \n",
        "# Adding our own layer \n",
        "x = GlobalAveragePooling2D()(base_model_ouput)\n",
        "# Adding fully connected layer\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dense(5, activation='softmax', name='fcnew')(x)\n",
        "    \n",
        "model = Model(inputs=base_model.input, outputs=x)"
      ],
      "metadata": {
        "id": "rxW2MmRkUMNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Epochs and Batch Size\n",
        "epochs = 3\n",
        "batch_size = 32\n",
        "\n",
        "# Compiling the Keras Model \n",
        "model.compile(optimizer=Adam(learning_rate=0.01), loss=\"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
        "\n",
        "# summary\n",
        "#model.summary()"
      ],
      "metadata": {
        "id": "DUYeEbCiUMQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "start = time()\n",
        "\n",
        "# Fitting on the Training set and making predcitons on the Validation set\n",
        "history = model.fit_generator(datagen.flow(x_train,y_train, batch_size = batch_size), \n",
        "                              epochs= epochs, \n",
        "                              validation_data=(x_test,y_test), \n",
        "                              verbose = 1, \n",
        "                              steps_per_epoch=x_train.shape[0] // batch_size)\n",
        "\n",
        "print(\"Time taken to train the Neural Net : \",time()-start)"
      ],
      "metadata": {
        "id": "1ZuGCJnVUMTC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d3362b4-c4e6-46b5-ff43-20642b5b2def"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-cb3adc43da2b>:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(datagen.flow(x_train,y_train, batch_size = batch_size),\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            " 98/107 [==========================>...] - ETA: 1:57 - loss: 1.1318 - accuracy: 0.6017"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MODEL NAME <---> \",model)\n",
        "plt.figure(figsize=(10, 5))\n",
        "  \n",
        "# summarize history for accuracy\n",
        "plt.subplot(1, 2 ,1)\n",
        "plt.plot(history.history[\"accuracy\"])\n",
        "plt.plot(history.history[\"val_accuracy\"])\n",
        "plt.xticks(np.arange(0, epochs+1, epochs/10))\n",
        "plt.title('Training Accuracy vs. Validation Accuracy')\n",
        "plt.xlabel('Num of Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['train', 'validation'], loc='best')\n",
        "  \n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.xticks(np.arange(0, epochs+1, epochs/10))\n",
        "plt.title('Training Loss vs. Validation Loss')\n",
        "plt.xlabel('Num of Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['train', 'validation'], loc='best')\n",
        "  \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CxsXpdmOUMVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "predictions = model.predict(x_test)\n",
        "predictions = np.argmax(predictions, axis = -1)\n",
        "predictions = predictions.reshape(1,-1)[0]\n",
        "\n",
        "labelEncoder.fit_transform(z_test)\n",
        "\n",
        "print(metrics.classification_report(labelEncoder.fit_transform(z_test), predictions))\n",
        "\n",
        "print(confusion_matrix(labelEncoder.fit_transform(z_test), predictions))\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(labelEncoder.fit_transform(z_test), predictions, cmap=plt.cm.Greens, display_labels=['daisy', 'dandelion', 'rose', 'sunflower', 'tulip'])\n",
        "plt.title('Confusion matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ao3hVAnlUF1h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}